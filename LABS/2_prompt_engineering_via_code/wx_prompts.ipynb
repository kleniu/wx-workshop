{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the third part of the lab dedicated to prompt engineering. This time we will query watsonx.ai from the code prepared in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from dotenv import load_dotenv\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Upload your credentials to the Watsonx.ai environment\n",
    "    - To obtain api_key, log in to your IBM Cloud account using the itz-watsonx user, then in the Manage tab select Access (IAM), on the left side of the list there will be an API Keys section, where you need to generate a new API key. The generated key should then be copied and completed in the api_key field below.\n",
    "\n",
    "    - As endpoint, use \"https://us-south.ml.cloud.ibm.com\".\n",
    "\n",
    "    - To get the project_id, enter the name of your project, then the Manage tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name\n",
    "#google/flan-ul2\n",
    "#google/flan-t5-xxl\n",
    "#ibm/mpt-7b-instruct2\n",
    "#EleutherAI/gpt-neox-20b\n",
    "\n",
    "#decoding_method\n",
    "#greedy\n",
    "#sampling\n",
    "\n",
    "#Set up your watsonx.ai environment\n",
    "my_credentials = {\n",
    "    \"url\" : \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\" : \"\"\n",
    "}\n",
    "project_id = \"\"\n",
    "\n",
    "def send_to_watsonxai(prompt,\n",
    "                    model_id=ModelTypes.FLAN_UL2,\n",
    "                    decoding_method=\"greedy\",\n",
    "                    max_new_tokens=100, #maximum value - 500\n",
    "                    min_new_tokens=30, #minimum value - 0\n",
    "                    temperature=1.0,\n",
    "                    repetition_penalty=2.0\n",
    "                    ):\n",
    "    '''\n",
    "   helper function for sending prompts and params to Watsonx.ai\n",
    "    \n",
    "    Args:  \n",
    "        prompts:list list of text prompts\n",
    "        decoding:str Watsonx.ai parameter \"sample\" or \"greedy\"\n",
    "        max_new_tok:int Watsonx.ai parameter for max new tokens/response returned\n",
    "        temp:float Watsonx.ai parameter for temperature (range 0>2)\n",
    "\n",
    "    Returns: None\n",
    "        prints response\n",
    "    '''\n",
    "\n",
    "    space_id = None\n",
    "    verify = False\n",
    "    \n",
    "    generate_params = {\n",
    "        GenParams.DECODING_METHOD: decoding_method,\n",
    "        GenParams.MAX_NEW_TOKENS: max_new_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_new_tokens,\n",
    "        GenParams.TEMPERATURE: temperature,\n",
    "        GenParams.REPETITION_PENALTY: repetition_penalty,\n",
    "        GenParams.RANDOM_SEED: 42\n",
    "    }\n",
    "\n",
    "    # Instantiate a model proxy object to send your requests\n",
    "    model = Model( model_id, my_credentials, generate_params, project_id, space_id, verify )   \n",
    "\n",
    "    result = model.generate(prompt)\n",
    "    print(result[\"results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1) A patient's a1c level determines his diabetic status, the rules are as follows:\n",
    "\n",
    " - less than 5.7 means no diabetes\n",
    " - between 5.7 and 6.5 prediabetes\n",
    " - more than 6.5 means diabetic condition.\n",
    "\n",
    "Based on the following three examples, write a prompt that will return the diabetes status of the described cases:\n",
    "\n",
    "1) The patients a1c is 5.5 which is good considering his other risk factors.\n",
    "2) From the last lab report I noted the A1c is 6.4 so we need to put her on Ozempic.\n",
    "3) She mentioned her A1c is 8 according to her blood work about 3 years ago.\n",
    "\n",
    "Bonus 1: How can you improve your inference by taking into account other information in the sentences?\n",
    "\n",
    "Bonus 2: How would you approach isolating diabetes status from patient notes without A1C values, and what would you need to watch out for? (hint: maybe they're talking about family history or other complications)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello hey hi hello ', 'generated_token_count': 100, 'input_token_count': 4, 'stop_reason': 'MAX_TOKENS'}]\n"
     ]
    }
   ],
   "source": [
    "#Q1 ENTER YOUR MODEL PARAMS HERE - MAKE SURE IT WORKS WITH ALL 3 EXAMPLES ABOVE\n",
    "\n",
    "\n",
    "prompt = \"\"\n",
    "\n",
    "\n",
    "\n",
    "response = send_to_watsonxai(prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product's review for task 2\n",
    "prompt = \"\"\"Needed a nice lamp for my bedroom, and this one had \\\n",
    "additional storage and not too high of a price point. \\\n",
    "Got it fast.  The string to our lamp broke during the \\\n",
    "transit and the company happily sent over a new one. \\\n",
    "Came within a few days as well. It was easy to put \\\n",
    "together.  I had a missing part, so I contacted their \\\n",
    "support and they very quickly got me the missing piece! \\\n",
    "Lumina seems to me to be a great company that cares \\\n",
    "about their customers and products!!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2) Write a prompt that will return the sentiment related to the given opinion\n",
    "Target sentiment = positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "#Q2 Code - enter prompt and parameters in this cell\n",
    "prompt = \"\"\n",
    "response = send_to_watsonxai(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}